import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urlparse, urljoin
from typing import Dict, List, Optional
import json
import logging

class WebScraper:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
    """
    
    def __init__(self):
        self.session = requests.Session()
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        self.site_rules = {
            'otomoto.pl': self._parse_otomoto,
            'youtube.com': self._parse_youtube,
            'youtu.be': self._parse_youtube,
            'reddit.com': self._parse_reddit,
            'wikipedia.org': self._parse_wikipedia,
            'drive2.ru': self._parse_drive2,
            'auto.ru': self._parse_auto_ru,
            'carsguru.net': self._parse_carsguru,
            'drom.ru': self._parse_drom,
            'motor.ru': self._parse_motor_ru,
            'autogeek.pl': self._parse_autogeek,
            'autokult.pl': self._parse_autokult,
            'whatcar.pl': self._parse_whatcar,
        }
        
    def scrape_url(self, url: str) -> Dict:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Dict —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme:
                url = 'https://' + url
                parsed_url = urlparse(url)
            
            domain = parsed_url.netloc.lower()
            
            print(f"üåê –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            result = {
                'url': url,
                'domain': domain,
                'title': self._get_title(soup),
                'description': self._get_description(soup),
                'content': '',
                'images': [],
                'links': [],
                'structured_data': {},
                'success': True,
                'error': None
            }
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
            site_handler = None
            for site_pattern, handler in self.site_rules.items():
                if site_pattern in domain:
                    site_handler = handler
                    break
            
            if site_handler:
                try:
                    structured_data = site_handler(soup, url)
                    result['structured_data'] = structured_data
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è {domain}: {e}")
            
            # –û–±—â–µ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            result['content'] = self._extract_general_content(soup)
            result['images'] = self._extract_images(soup, url)[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            result['links'] = self._extract_links(soup, url)[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å—Å—ã–ª–æ–∫
            
            return result
            
        except requests.exceptions.RequestException as e:
            return {
                'url': url,
                'success': False,
                'error': f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}',
                'content': None
            }
        except Exception as e:
            return {
                'url': url,
                'success': False,
                'error': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}',
                'content': None
            }
    
    def _get_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        return "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    def _get_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ meta —Ç–µ–≥–æ–≤"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '').strip()
        
        # –ü—Ä–æ–±—É–µ–º Open Graph
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc:
            return og_desc.get('content', '').strip()
        
        return "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    def _extract_general_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
        
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
        
        # –û—á–∏—â–∞–µ–º –∏ —Å–æ–∫—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç
        text = re.sub(r'\s+', ' ', text)
        return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö"""
        images = []
        for img in soup.find_all('img', src=True):
            src = img['src']
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = urljoin(base_url, src)
            
            images.append({
                'url': src,
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        return images
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏"""
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('http'):
                links.append({
                    'url': href,
                    'text': link.get_text(strip=True),
                    'title': link.get('title', '')
                })
        
        return links
    
    def _parse_otomoto(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è OtoMoto"""
        data = {
            'type': 'car_listing',
            'price': None,
            'car_details': {},
            'seller_info': {},
            'description': '',
            'images': [],
            'main_image': None
        }
        
        # –¶–µ–Ω–∞
        price_elem = soup.find('span', class_='offer-price__number')
        if price_elem:
            data['price'] = price_elem.get_text().strip()
        
        # –î–µ—Ç–∞–ª–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_items = soup.find_all('li', class_='offer-params__item')
        for item in param_items:
            label_elem = item.find('span', class_='offer-params__label')
            value_elem = item.find('div', class_='offer-params__value') or item.find('a', class_='offer-params__value')
            
            if label_elem and value_elem:
                label = label_elem.get_text().strip().rstrip(':')
                value = value_elem.get_text().strip()
                data['car_details'][label] = value
        
        # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–≤—Ü–∞
        desc_elem = soup.find('div', class_='offer-description__description')
        if desc_elem:
            data['description'] = desc_elem.get_text().strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        images = []
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        main_img = soup.find('img', class_='bigImage')
        if not main_img:
            main_img = soup.find('img', attrs={'data-src': True})
        if not main_img:
            # –ü–æ–∏—Å–∫ –ª—é–±—ã—Ö –±–æ–ª—å—à–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–∞–ª–µ—Ä–µ–µ
            gallery_imgs = soup.find_all('img', attrs={'src': re.compile(r'.*\.(jpg|jpeg|png|webp)', re.I)})
            for img in gallery_imgs:
                src = img.get('src') or img.get('data-src')
                if src and 'placeholder' not in src.lower() and 'logo' not in src.lower():
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://otomoto.pl' + src
                    
                    images.append({
                        'url': src,
                        'alt': img.get('alt', ''),
                        'title': 'Car image'
                    })
        
        if main_img:
            main_src = main_img.get('src') or main_img.get('data-src')
            if main_src:
                if main_src.startswith('//'):
                    main_src = 'https:' + main_src
                elif main_src.startswith('/'):
                    main_src = 'https://otomoto.pl' + main_src
                
                data['main_image'] = main_src
                images.insert(0, {
                    'url': main_src,
                    'alt': main_img.get('alt', ''),
                    'title': 'Main car image'
                })
        
        data['images'] = images[:6]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 6 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        
        return data
    
    def _parse_youtube(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è YouTube"""
        data = {
            'type': 'youtube_video',
            'video_title': '',
            'channel': '',
            'description': '',
            'views': '',
            'upload_date': ''
        }
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∏–¥–µ–æ
        title_elem = soup.find('meta', {'property': 'og:title'})
        if title_elem:
            data['video_title'] = title_elem.get('content', '')
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_elem = soup.find('meta', {'property': 'og:description'})
        if desc_elem:
            data['description'] = desc_elem.get('content', '')
        
        return data
    
    def _parse_reddit(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Reddit"""
        data = {
            'type': 'reddit_post',
            'title': '',
            'subreddit': '',
            'author': '',
            'content': ''
        }
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Å—Ç–∞
        title_elem = soup.find('h1')
        if title_elem:
            data['title'] = title_elem.get_text().strip()
        
        return data
    
    def _parse_wikipedia(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Wikipedia"""
        data = {
            'type': 'wikipedia_article',
            'title': '',
            'summary': '',
            'categories': []
        }
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
        title_elem = soup.find('h1', class_='firstHeading')
        if title_elem:
            data['title'] = title_elem.get_text().strip()
        
        # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü)
        first_para = soup.find('div', class_='mw-parser-output')
        if first_para:
            para = first_para.find('p')
            if para:
                data['summary'] = para.get_text().strip()
        
        return data
    
    def _parse_drive2(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Drive2.ru"""
        data = {
            'type': 'drive2_review',
            'title': '',
            'author': '',
            'car_model': '',
            'review_text': '',
            'rating': None,
            'pros_cons': {'pros': [], 'cons': []}
        }
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Å—Ç–∞
        title_elem = soup.find('h1', class_='c-title')
        if title_elem:
            data['title'] = title_elem.get_text().strip()
        
        # –ê–≤—Ç–æ—Ä
        author_elem = soup.find('a', class_='c-username')
        if author_elem:
            data['author'] = author_elem.get_text().strip()
        
        # –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
        content_elem = soup.find('div', class_='c-post-text')
        if content_elem:
            data['review_text'] = content_elem.get_text()[:2000]
        
        return data
    
    def _parse_auto_ru(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Auto.ru"""
        data = {
            'type': 'auto_ru_listing',
            'price': None,
            'car_details': {},
            'description': '',
            'location': ''
        }
        
        # –¶–µ–Ω–∞
        price_elem = soup.find('span', class_='OfferPriceCaption__price')
        if price_elem:
            data['price'] = price_elem.get_text().strip()
        
        return data
    
    def _parse_carsguru(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è CarsGuru.net"""
        data = {
            'type': 'carsguru_review',
            'car_model': '',
            'overall_rating': None,
            'category_ratings': {},
            'review_summary': ''
        }
        
        return data
    
    def _parse_drom(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Drom.ru"""
        data = {
            'type': 'drom_listing',
            'price': None,
            'car_details': {},
            'seller_type': ''
        }
        
        return data
    
    def _parse_motor_ru(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Motor.ru"""
        data = {
            'type': 'motor_review',
            'article_type': '',
            'car_model': '',
            'expert_opinion': '',
            'test_results': {}
        }
        
        return data
    
    def _parse_autogeek(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è AutoGeek.pl"""
        data = {
            'type': 'autogeek_article',
            'title': '',
            'category': '',
            'content': ''
        }
        
        return data
    
    def _parse_autokult(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è AutoKult.pl"""
        data = {
            'type': 'autokult_review',
            'title': '',
            'car_model': '',
            'rating': None
        }
        
        return data
    
    def _parse_whatcar(self, soup: BeautifulSoup, url: str) -> Dict:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è WhatCar.pl"""
        data = {
            'type': 'whatcar_review',
            'title': '',
            'expert_rating': None,
            'user_rating': None
        }
        
        return data

class ReviewSearcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è—Ö"""
    
    def __init__(self):
        self.scraper = WebScraper()
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ–æ—Ä—É–º—ã –∏ —Å–∞–π—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤
        self.review_sites = {
            'youtube': {
                'base_url': 'https://www.youtube.com/results?search_query=',
                'keywords': ['review', 'test', '–æ–±–∑–æ—Ä', '—Ç–µ—Å—Ç-–¥—Ä–∞–π–≤', '–æ—Ç–∑—ã–≤ –≤–ª–∞–¥–µ–ª—å—Ü–∞']
            },
            'drive2': {
                'base_url': 'https://www.drive2.ru/search/?q=',
                'keywords': ['–æ—Ç–∑—ã–≤', '–æ–ø—ã—Ç —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏', '–≥–æ–¥ —Å –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–º']
            },
            'reddit': {
                'base_url': 'https://www.reddit.com/search/?q=',
                'keywords': ['review', 'owner experience', 'reliability']
            }
        }
    
    def search_reviews_for_car(self, car_make: str, car_model: str, year: str = '') -> Dict:
        """–ü–æ–∏—Å–∫ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è"""
        print(f"üîç –ò—â—É –æ—Ç–∑—ã–≤—ã –¥–ª—è {car_make} {car_model} {year}")
        
        search_queries = self._generate_search_queries(car_make, car_model, year)
        all_results = {
            'found_reviews': [],
            'summary': {
                'total_found': 0,
                'youtube_reviews': 0,
                'forum_posts': 0,
                'expert_reviews': 0
            },
            'common_issues': [],
            'positive_feedback': [],
            'overall_sentiment': 'neutral'
        }
        
        # –ó–¥–µ—Å—å –±—ã –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–ª–∞–ª–∏—Å—å –∑–∞–ø—Ä–æ—Å—ã –∫ –ø–æ–∏—Å–∫–æ–≤—ã–º API
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º mock –¥–∞–Ω–Ω—ã–µ
        mock_reviews = self._generate_mock_reviews(car_make, car_model, year)
        all_results.update(mock_reviews)
        
        return all_results
    
    def _generate_search_queries(self, make: str, model: str, year: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º"""
        base_terms = [f"{make} {model}", f"{make} {model} {year}"] if year else [f"{make} {model}"]
        
        queries = []
        for term in base_terms:
            queries.extend([
                f"{term} review",
                f"{term} –æ—Ç–∑—ã–≤ –≤–ª–∞–¥–µ–ª—å—Ü–∞",
                f"{term} problems issues",
                f"{term} –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å",
                f"{term} test drive",
                f"{term} owner experience"
            ])
        
        return queries
    
    def _generate_mock_reviews(self, make: str, model: str, year: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç mock –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª–∏ –±—ã –¥–∞–Ω–Ω—ã–µ —Å YouTube API, Reddit API, etc.
        return {
            'found_reviews': [
                {
                    'source': 'YouTube',
                    'title': f'{make} {model} - –ß–µ—Å—Ç–Ω—ã–π –æ—Ç–∑—ã–≤ –≤–ª–∞–¥–µ–ª—å—Ü–∞ –ø–æ—Å–ª–µ –≥–æ–¥–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏',
                    'author': 'AutoReviewRU',
                    'rating': 4.2,
                    'key_points': ['–ù–∞–¥–µ–∂–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å', '–î–æ—Ä–æ–≥–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', '–û—Ç–ª–∏—á–Ω–∞—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å'],
                    'url': 'mock_youtube_url',
                    'views': '125K'
                },
                {
                    'source': 'Drive2.ru',
                    'title': f'–ú–æ–π –æ–ø—ã—Ç —Å {make} {model} - –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã',
                    'author': '–í–ª–∞–¥–µ–ª–µ—Ü –∏–∑ –ú–æ—Å–∫–≤—ã',
                    'rating': 3.8,
                    'key_points': ['–ö–æ–º—Ñ–æ—Ä—Ç–Ω—ã–π —Å–∞–ª–æ–Ω', '–ü—Ä–æ–±–ª–µ–º—ã —Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–æ–π', '–í—ã—Å–æ–∫–∏–π —Ä–∞—Å—Ö–æ–¥'],
                    'url': 'mock_drive2_url',
                    'views': '45K'
                },
                {
                    'source': 'Reddit',
                    'title': f'{make} {model} reliability discussion',
                    'author': 'r/cars community',
                    'rating': 4.0,
                    'key_points': ['Generally reliable', 'Some transmission issues', 'Great performance'],
                    'url': 'mock_reddit_url',
                    'views': '200 comments'
                }
            ],
            'summary': {
                'total_found': 3,
                'youtube_reviews': 1,
                'forum_posts': 2,
                'expert_reviews': 0
            },
            'common_issues': [
                '–î–æ—Ä–æ–≥–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –∏ –∑–∞–ø—á–∞—Å—Ç–∏',
                '–ü—Ä–æ–±–ª–µ–º—ã —Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–æ–π –ø–æ—Å–ª–µ 3-4 –ª–µ—Ç',
                '–í—ã—Å–æ–∫–∏–π —Ä–∞—Å—Ö–æ–¥ —Ç–æ–ø–ª–∏–≤–∞ –≤ –≥–æ—Ä–æ–¥–µ'
            ],
            'positive_feedback': [
                '–û—Ç–ª–∏—á–Ω–∞—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –∏ –¥–∏–Ω–∞–º–∏–∫–∞',
                '–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—å–µ—Ä',
                '–ü—Ä–µ—Å—Ç–∏–∂–Ω—ã–π –±—Ä–µ–Ω–¥ –∏ –≤—ã—Å–æ–∫–∞—è –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å',
                '–ù–∞–¥–µ–∂–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏'
            ],
            'overall_sentiment': 'positive'
        }

class CarAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""
    
    def __init__(self):
        self.review_searcher = ReviewSearcher()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Å –≤–µ—Å–∞–º–∏
        self.evaluation_criteria = {
            'reliability': {'weight': 0.25, 'description': '–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å'},
            'comfort': {'weight': 0.20, 'description': '–ö–æ–º—Ñ–æ—Ä—Ç'},
            'performance': {'weight': 0.20, 'description': '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'},
            'economy': {'weight': 0.15, 'description': '–≠–∫–æ–Ω–æ–º–∏—á–Ω–æ—Å—Ç—å'},
            'safety': {'weight': 0.10, 'description': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'},
            'design': {'weight': 0.10, 'description': '–î–∏–∑–∞–π–Ω'}
        }
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–¥–µ–ª–µ–π (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –±—ã–ª–∞ –±—ã –∏–∑ –±–∞–∑—ã)
        self.model_ratings = {
            'mercedes_gle': {
                'reliability': 3.8,
                'comfort': 4.7,
                'performance': 4.5,
                'economy': 3.2,
                'safety': 4.8,
                'design': 4.6
            },
            'bmw_3_series': {
                'reliability': 4.1,
                'comfort': 4.3,
                'performance': 4.7,
                'economy': 4.0,
                'safety': 4.5,
                'design': 4.4
            }
        }
    
    def analyze_car_from_listing(self, listing_data: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–≤—Ç–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        if not listing_data.get('structured_data') or listing_data['structured_data'].get('type') != 'car_listing':
            return None
        
        car_data = listing_data['structured_data']
        analysis = {
            'overall_score': 0,
            'category_scores': {},
            'price_analysis': {},
            'recommendation': '',
            'pros_cons': {'pros': [], 'cons': []},
            'market_comparison': {},
            'depreciation_forecast': {}
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        model_key = self._identify_model(listing_data.get('title', '').lower())
        
        if model_key and model_key in self.model_ratings:
            ratings = self.model_ratings[model_key]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            total_weighted_score = 0
            for criterion, config in self.evaluation_criteria.items():
                score = ratings.get(criterion, 3.0)
                analysis['category_scores'][criterion] = {
                    'score': score,
                    'description': config['description'],
                    'weight': config['weight']
                }
                total_weighted_score += score * config['weight']
            
            analysis['overall_score'] = round(total_weighted_score, 1)
            analysis['recommendation'] = self._generate_recommendation(analysis['overall_score'])
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω—ã
        if car_data.get('price'):
            analysis['price_analysis'] = self._analyze_price(car_data)
        
        return analysis
    
    def compare_cars(self, car1_data: Dict, car2_data: Dict) -> Dict:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è"""
        analysis1 = self.analyze_car_from_listing(car1_data)
        analysis2 = self.analyze_car_from_listing(car2_data)
        
        if not analysis1 or not analysis2:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω –∏–∑ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π'}
        
        comparison = {
            'winner': None,
            'score_difference': abs(analysis1['overall_score'] - analysis2['overall_score']),
            'category_comparison': {},
            'summary': '',
            'car1_advantages': [],
            'car2_advantages': [],
            'final_recommendation': ''
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        if analysis1['overall_score'] > analysis2['overall_score']:
            comparison['winner'] = 'car1'
        elif analysis2['overall_score'] > analysis1['overall_score']:
            comparison['winner'] = 'car2'
        else:
            comparison['winner'] = 'tie'
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for criterion in self.evaluation_criteria.keys():
            score1 = analysis1['category_scores'][criterion]['score']
            score2 = analysis2['category_scores'][criterion]['score']
            
            comparison['category_comparison'][criterion] = {
                'car1_score': score1,
                'car2_score': score2,
                'difference': score1 - score2,
                'winner': 'car1' if score1 > score2 else 'car2' if score2 > score1 else 'tie'
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
            if score1 > score2 + 0.3:
                comparison['car1_advantages'].append(self.evaluation_criteria[criterion]['description'])
            elif score2 > score1 + 0.3:
                comparison['car2_advantages'].append(self.evaluation_criteria[criterion]['description'])
        
        comparison['final_recommendation'] = self._generate_final_recommendation(comparison)
        
        return comparison
    
    def _identify_model(self, title: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∞–≤—Ç–æ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        if 'mercedes' in title and 'gle' in title:
            return 'mercedes_gle'
        elif 'bmw' in title and ('3' in title or 'series' in title):
            return 'bmw_3_series'
        return None
    
    def _analyze_price(self, car_data: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –∞–≤—Ç–æ–º–æ–±–∏–ª—è"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –±—ã–ª –±—ã —Å–ª–æ–≤–∞—Ä—å —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ü–µ–Ω
        price_str = car_data.get('price', '0')
        try:
            price_num = float(''.join(filter(str.isdigit, price_str)))
        except:
            price_num = 0
        
        return {
            'price_value': price_num,
            'market_position': 'average' if 100000 <= price_num <= 300000 else 'premium' if price_num > 300000 else 'budget',
            'value_rating': 3.5,  # –ú–æ–∫-–¥–∞–Ω–Ω—ã–µ
            'negotiation_potential': '5-10%'
        }
    
    def _generate_recommendation(self, score: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏"""
        if score >= 4.5:
            return '–û—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä! –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∫ –ø–æ–∫—É–ø–∫–µ.'
        elif score >= 4.0:
            return '–•–æ—Ä–æ—à–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞–º–∏.'
        elif score >= 3.5:
            return '–°—Ä–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã.'
        else:
            return '–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∫ –ø–æ–∫—É–ø–∫–µ. –ú–Ω–æ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤.'
    
    def _generate_final_recommendation(self, comparison: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é"""
        if comparison['winner'] == 'tie':
            return '–û–±–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É. –í—ã–±–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ª–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π.'
        elif comparison['score_difference'] < 0.3:
            return '–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ. –û–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–æ—Å—Ç–æ–π–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è.'
        elif comparison['score_difference'] < 0.7:
            winner = '–ü–µ—Ä–≤—ã–π' if comparison['winner'] == 'car1' else '–í—Ç–æ—Ä–æ–π'
            return f'{winner} –∞–≤—Ç–æ–º–æ–±–∏–ª—å –∏–º–µ–µ—Ç –∑–∞–º–µ—Ç–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ.'
        else:
            winner = '–ü–µ—Ä–≤—ã–π' if comparison['winner'] == 'car1' else '–í—Ç–æ—Ä–æ–π'
            return f'{winner} –∞–≤—Ç–æ–º–æ–±–∏–ª—å —è–≤–Ω–æ –ª—É—á—à–µ. –û—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä!'

def detect_urls_in_text(text: str) -> List[str]:
    """
    –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç URL –≤ —Ç–µ–∫—Å—Ç–µ
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL
    """
    url_pattern = re.compile(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        r'|(?:www\.)?[a-zA-Z0-9][-a-zA-Z0-9]*[a-zA-Z0-9]*\.(?:[a-zA-Z]{2,4})(?:[/?][-a-zA-Z0-9._~:/?#[@!$&\'()*+,;=]*)?'
    )
    
    urls = url_pattern.findall(text)
    
    # –î–æ–±–∞–≤–ª—è–µ–º https:// –∫ URL –±–µ–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
    normalized_urls = []
    for url in urls:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        normalized_urls.append(url)
    
    return normalized_urls